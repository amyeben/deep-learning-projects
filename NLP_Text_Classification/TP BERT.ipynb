{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57ea29d-27c1-4ff9-bb23-7e62aecf6836",
   "metadata": {},
   "source": [
    "### TP Transformer BERT : Classification de texte avec BERT\n",
    "\n",
    "`Mélissa Ben Saada`\n",
    "`Amy EBEN SANG KOTTA`\n",
    "\n",
    "#### **Objectif** :\n",
    "Utiliser trois modèles BERT différents pour effectuer une tâche de classification de texte (analyse de sentiment) et comparer leurs performances à l'aide de métriques.\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Importation des bibliothèques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d9426c-5ce8-40cf-a2a5-8eaf21173bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:48:50.374655: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01493c9-bfb7-4549-b13f-d8ed1b068a80",
   "metadata": {},
   "source": [
    "#### **2. Chargement des modèles et des tokenizers**\n",
    "Sélectionnons trois modèles BERT différents :\n",
    "1. **`distilbert-base-uncased-finetuned-sst-2-english`** (une version allégée de BERT)\n",
    "2. **`distilbert-base-uncased`** (une version allégée de BERT)\n",
    "3. **`roberta-base`** (basé sur BERT mais avec des optimisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4524072-adba-48f0-8188-f336e43a881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Chargement des modèles et création des pipelines\n",
    "models = {\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\": pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "    \"distilbert-base-uncased\": pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased\"),\n",
    "    \"roberta-base\": pipeline(\"sentiment-analysis\", model=\"roberta-base\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258013f2-3094-452e-b570-9c6277c1c50a",
   "metadata": {},
   "source": [
    "#### **3. Création d'un pipeline pour la classification de texte**\n",
    "Testons chaque modèle avec un ensemble d'exemples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823ee9a9-65d8-4076-8b2e-4b43eb8366da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for distilbert-base-uncased-finetuned-sst-2-english:\n",
      "Example 1: {'label': 'POSITIVE', 'score': 0.9998830556869507}\n",
      "Example 2: {'label': 'NEGATIVE', 'score': 0.9998016953468323}\n",
      "Example 3: {'label': 'POSITIVE', 'score': 0.5213127136230469}\n",
      "\n",
      "Results for distilbert-base-uncased:\n",
      "Example 1: {'label': 'LABEL_0', 'score': 0.5148850679397583}\n",
      "Example 2: {'label': 'LABEL_0', 'score': 0.5238295197486877}\n",
      "Example 3: {'label': 'LABEL_0', 'score': 0.5196446776390076}\n",
      "\n",
      "Results for roberta-base:\n",
      "Example 1: {'label': 'LABEL_1', 'score': 0.5098674297332764}\n",
      "Example 2: {'label': 'LABEL_1', 'score': 0.512851893901825}\n",
      "Example 3: {'label': 'LABEL_1', 'score': 0.511315643787384}\n"
     ]
    }
   ],
   "source": [
    "# Phrases d'exemple\n",
    "examples = [\n",
    "    \"I love this product! It works perfectly and is exactly what I needed.\",\n",
    "    \"This is the worst experience I've ever had. Completely disappointed.\",\n",
    "    \"The quality is decent for the price, but I expected better.\"\n",
    "]\n",
    "\n",
    "# Analyser les phrases avec chaque modèle\n",
    "results = {}\n",
    "for model_name, classifier in models.items():\n",
    "    results[model_name] = [classifier(text)[0] for text in examples]\n",
    "\n",
    "# Affichage des résultats\n",
    "for model_name, output in results.items():\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    for i, result in enumerate(output):\n",
    "        print(f\"Example {i+1}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8705baa-6c0c-4a3c-af2f-c2ade19c80bd",
   "metadata": {},
   "source": [
    "#### **4. Évaluation des performances avec des métriques**\n",
    "Créons un jeu de données de test avec des labels (0 pour négatif, 1 pour positif) et comparons les prédictions des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bf598d-afcd-4c51-984e-36702242159b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating distilbert-base-uncased-finetuned-sst-2-english...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       1.00      1.00      1.00         1\n",
      "    POSITIVE       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "\n",
      "Evaluating distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.33      1.00      0.50         1\n",
      "    POSITIVE       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n",
      "\n",
      "Evaluating roberta-base...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.33      1.00      0.50         1\n",
      "    POSITIVE       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Données étiquetées\n",
    "test_data = [\n",
    "    {\"text\": \"I love this product! It works perfectly and is exactly what I needed.\", \"label\": 1},\n",
    "    {\"text\": \"This is the worst experience I've ever had. Completely disappointed.\", \"label\": 0},\n",
    "    {\"text\": \"The quality is decent for the price, but I expected better.\", \"label\": 1},\n",
    "]\n",
    "\n",
    "# Évaluer chaque modèle\n",
    "for model_name, classifier in models.items():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    y_true = [data[\"label\"] for data in test_data]\n",
    "    y_pred = []\n",
    "    for data in test_data:\n",
    "        prediction = classifier(data[\"text\"])[0][\"label\"]\n",
    "        y_pred.append(1 if prediction == \"POSITIVE\" else 0)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"NEGATIVE\", \"POSITIVE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe89852-19e2-472a-8267-02c1906761b4",
   "metadata": {},
   "source": [
    "### **1. distilbert-base-uncased-finetuned-sst-2-english**\n",
    "\n",
    "Le modèle **`distilbert-base-uncased-finetuned-sst-2-english`** est une version allégée de BERT, appelée **DistilBERT**, fine-tunée spécifiquement pour l’analyse de sentiments. Ce fine-tuning a été effectué sur le dataset **SST-2 (Stanford Sentiment Treebank version 2)**, un ensemble de données contenant des critiques de films annotées comme **positives** ou **négatives**. Par exemple :\n",
    "- \"A wonderful little film about love.\" → **POSITIVE**\n",
    "- \"An absolute mess.\" → **NEGATIVE**\n",
    "\n",
    "**DistilBERT** a été préentraîné sur des données textuelles massives comme **Wikipedia** et le **BookCorpus**, deux sources riches en contenu généraliste. Ce modèle est particulièrement adapté pour des tâches comme l’analyse de sentiments sans nécessiter de modification supplémentaire, grâce à son fine-tuning sur SST-2.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. distilbert-base-uncased**\n",
    "\n",
    "Le modèle **`distilbert-base-uncased`** est une version légère et optimisée de BERT. Il ne fait pas la distinction entre les majuscules et les minuscules (**uncased**) et est conçu pour être plus rapide et moins coûteux en ressources tout en conservant une grande partie des performances de BERT. Contrairement à la version fine-tunée sur SST-2, celui-ci n’est pas spécialisé dans une tâche précise.\n",
    "\n",
    "Ce modèle a été préentraîné sur des données textuelles massives comme **Wikipedia** et le **BookCorpus**, ce qui lui permet de comprendre le langage de manière générale. Cependant, pour des tâches spécifiques comme l’analyse de sentiments, ce modèle nécessite un **fine-tuning** supplémentaire pour apprendre les nuances des données spécifiques.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. roberta-base**\n",
    "\n",
    "Le modèle **`roberta-base`** est une version optimisée et plus robuste de BERT, développée par Facebook AI. Contrairement à BERT, RoBERTa a été préentraîné sur un corpus de données beaucoup plus grand, comprenant :\n",
    "- **Wikipedia**\n",
    "- **BookCorpus**\n",
    "- **Common Crawl** : un ensemble massif de données textuelles collectées sur le Web.\n",
    "- **OpenWebText** : des textes issus de liens partagés sur Reddit.\n",
    "- **Stories** : des textes narratifs extraits de sites comme Gutenberg.\n",
    "\n",
    "RoBERTa se distingue par plusieurs optimisations, comme l’utilisation de séquences plus longues et la suppression des tokens de segmentation. Cela le rend plus performant pour des tâches variées, mais il nécessite un **fine-tuning** pour atteindre une performance optimale sur des tâches spécifiques, comme l’analyse de sentiments.\n",
    "\n",
    "---\n",
    "\n",
    "### **Résumé des sources de données**\n",
    "Tous ces modèles partagent des données communes pour leur préentraînement, principalement **Wikipedia** et **BookCorpus**, ce qui leur permet de développer une compréhension générale du langage. Les données additionnelles et les fine-tunings, comme SST-2 pour `distilbert-base-uncased-finetuned-sst-2-english`, les rendent spécialisés pour des tâches précises comme l’analyse de sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6265e3-0a66-40b4-bf1c-00365f47d0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
